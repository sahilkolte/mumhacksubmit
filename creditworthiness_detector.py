# -*- coding: utf-8 -*-
"""Creditworthiness Detector

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KCwGVGGkgvrla3_72eULwjalEGIMwbzk
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# %matplotlib inline
# %matplotlib notebook
plt.rcParams["figure.figsize"] = (12, 6)
# plt.rcParams['figure.dpi'] = 100
sns.set_style("whitegrid")
import warnings

warnings.filterwarnings("ignore")
warnings.warn("this will not show")
pd.set_option('display.float_format', lambda x: '%.3f' % x)

#--------------------
from sklearn.preprocessing import OrdinalEncoder, LabelEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.model_selection import train_test_split
from imblearn.over_sampling import SMOTE
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import classification_report,confusion_matrix

df_initial = pd.read_csv("/content/drive/MyDrive/Credit_Score_Clean.csv")
df = df_initial.copy()

df.head(5)

df.describe(include="object").T

df.info()

categories = ['Poor', 'Standard', 'Good']

encoder = OrdinalEncoder(categories=[categories])

df['Credit_Score'] = encoder.fit_transform(df[['Credit_Score']])

label_encoder = LabelEncoder()
df['Occupation'] = label_encoder.fit_transform(df['Occupation'])

categories = ['Bad', 'Standard', 'Good']

encoder = OrdinalEncoder(categories=[categories])

df['Credit_Mix'] = encoder.fit_transform(df[['Credit_Mix']])

categories_payment_behaviour = [
    'Low_spent_Small_value_payments',
    'Low_spent_Medium_value_payments',
    'Low_spent_Large_value_payments',
    'High_spent_Small_value_payments',
    'High_spent_Medium_value_payments',
    'High_spent_Large_value_payments'
]

encoder_payment_behaviour = OrdinalEncoder(categories=[categories_payment_behaviour])

df['Payment_Behaviour'] = encoder_payment_behaviour.fit_transform(df[['Payment_Behaviour']])

label_encoder = LabelEncoder()
df['Payment_of_Min_Amount'] = label_encoder.fit_transform(df['Payment_of_Min_Amount'])

X = df.drop(["Credit_Score"],axis=1)
y = df["Credit_Score"]

y.value_counts(normalize=True) #unbalanced

smote = SMOTE() # Synthetic Minority Oversampling Technique
X, y = smote.fit_resample(X,y)

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    stratify=y,
                                                    test_size=0.15,
                                                    random_state=42)

scaler = MinMaxScaler()

X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

from sklearn.model_selection import GridSearchCV

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Dense, Activation, BatchNormalization,Dropout
from tensorflow.keras.optimizers import Adam

model = Sequential()

model.add(Dense(128, activation="relu"))
model.add(BatchNormalization())
model.add(Dense(128, activation="relu"))
model.add(BatchNormalization())
model.add(Dropout(0.2))
model.add(Dense(64, activation="relu"))
model.add(BatchNormalization())
model.add(Dropout(0.1))
model.add(Dense(32, activation="relu"))
model.add(BatchNormalization())
model.add(Dense(16, activation="relu"))
model.add(BatchNormalization())
model.add(Dense(8, activation="relu"))
model.add(BatchNormalization())
model.add(Dense(3, activation="softmax")) #we use 3 output and softmax

opt = Adam(learning_rate=0.002)
model.compile(optimizer=opt,
              loss="sparse_categorical_crossentropy",
              metrics=["Accuracy"])

early_stop = EarlyStopping(monitor="val_loss", #ofcourse early_stop :)
                           mode="auto",
                           verbose=1,
                           patience=50)

model.fit(X_train,
          y_train,
          validation_split=.1,
          batch_size=256,
          epochs=500,
          verbose=1,
          callbacks=[early_stop])

def eval_metric(model, X_train, y_train, X_test, y_test):
    y_train_pred_probabilities = model.predict(X_train)
    y_train_pred = y_train_pred_probabilities.argmax(axis=1)
    y_pred_probabilities = model.predict(X_test)
    y_pred = y_pred_probabilities.argmax(axis=1)

    print("\n")

    print("\nTrain Set:")
    print(confusion_matrix(y_train, y_train_pred))
    print(classification_report(y_train, y_train_pred))

    print("Test Set:")
    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))

eval_metric(model, X_train, y_train, X_test, y_test)

model = Sequential()

model.add(Dense(256, activation="relu"))
model.add(BatchNormalization())
model.add(Dropout(0.1))
model.add(Dense(128, activation="relu"))
model.add(BatchNormalization())
model.add(Dense(64, activation="relu"))
model.add(BatchNormalization())
model.add(Dropout(0.1))
model.add(Dense(32, activation="relu"))
model.add(BatchNormalization())
model.add(Dense(16, activation="relu"))
model.add(BatchNormalization())
model.add(Dense(8, activation="relu"))
model.add(BatchNormalization())
model.add(Dense(3, activation="softmax")) #we use 3 output and softmax

opt = Adam(learning_rate=0.004)
model.compile(optimizer=opt,
              loss="sparse_categorical_crossentropy",
              metrics=["Accuracy"])

early_stop = EarlyStopping(monitor="val_loss", #ofcourse early_stop :)
                           mode="auto",
                           verbose=1,
                           patience=90)

model.fit(X_train,
          y_train,
          validation_split=.1,
          batch_size=512,
          epochs=500,
          verbose=1,
          callbacks=[early_stop])

eval_metric(model, X_train, y_train, X_test, y_test)

!pip install lightgbm

from lightgbm import LGBMClassifier

lgbm_model = LGBMClassifier(random_state=42)

lgbm_model.fit(X_train,y_train)

def eval_metric(model, X_train, y_train, X_test, y_test):
    y_train_pred = model.predict(X_train)
    y_pred = model.predict(X_test)

    print("Test_Set")
    print(confusion_matrix(y_test, y_pred))
    print(classification_report(y_test, y_pred))
    print()
    print("Train_Set")
    print(confusion_matrix(y_train, y_train_pred))
    print(classification_report(y_train, y_train_pred))

eval_metric(lgbm_model, X_train, y_train, X_test, y_test)

from sklearn.model_selection import GridSearchCV

param_grid = {
    'learning_rate': [0.1,0.5, 1],
    'n_estimators': [100, 200],
    'num_leaves': [20, 30, 50],
    'max_depth': [-1, 3, 4],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

param_grid = {
    'learning_rate': [0.1,0.5, 1],
    'n_estimators': [100, 200],
    'num_leaves': [20, 30, 50],
    'max_depth': [-1, 3, 4],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

lgbm_grid = GridSearchCV(estimator=lgbm_model,
                         param_grid=param_grid,
                         cv=3,
                         scoring='accuracy',
                         n_jobs=-1,
                         verbose=2,
                         return_train_score=True)

lgbm_grid = GridSearchCV(estimator=lgbm_model,
                         param_grid=param_grid,
                         cv=3,
                         scoring='accuracy',
                         n_jobs=1,
                         verbose=2,
                         return_train_score=True)

lgbm_grid.fit(X_train,y_train)

lgbm_grid.best_score_

eval_metric(lgbm_grid, X_train, y_train, X_test, y_test)

import pickle

# Assuming lgbm_grid is your trained model
model = lgbm_grid  # Assign the trained model to the 'model' variable

# Save your model
with open('credit_score_model.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)

!pip install streamlit
!pip install openai  # For OpenAI API integration (if you plan to use it)

import pickle

# Save your model (assuming you have a model named `model`)
with open('credit_score_model.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)

from joblib import dump

# Save your model (assuming you have a model named `model`)
dump(model, 'credit_score_model.joblib')

import streamlit as st
import pickle
import numpy as np

# Load your saved model
with open('credit_score_model.pkl', 'rb') as model_file:
    model = pickle.load(model_file)

# Define your prediction function
def predict_credit_score(input_data):
    # Convert the input to the format the model expects (e.g., numpy array)
    data = np.array(input_data).reshape(1, -1)
    prediction = model.predict(data)
    return prediction

# Streamlit App
st.title('Credit Score Prediction App')

# Input fields for transaction data (as an example, you can add more categories)
luxury_spending = st.number_input('Luxury Spending', min_value=0)
charity_spending = st.number_input('Charity Spending', min_value=0)
local_business_spending = st.number_input('Small/Local Business Spending', min_value=0)
groceries = st.number_input('Groceries/Food Spending', min_value=0)
transportation = st.number_input('Transportation Spending', min_value=0)
clothing_shopping = st.number_input('Clothing/Shopping Spending', min_value=0)
electronics = st.number_input('Electronics Spending', min_value=0)
utilities = st.number_input('Utilities Spending', min_value=0)
entertainment = st.number_input('Entertainment Spending', min_value=0)
other_services = st.number_input('Other Services Spending', min_value=0)

# Button to predict the credit score
if st.button('Predict Credit Score'):
    # Assuming your model takes 10 inputs (customize based on your model)
    input_data = [
        luxury_spending,
        charity_spending,
        local_business_spending,
        groceries,
        transportation,
        clothing_shopping,
        electronics,
        utilities,
        entertainment,
        other_services
    ]

    # Predict the credit score
    prediction = predict_credit_score(input_data)

    # Display the result
    st.success(f'Predicted Credit Score: {prediction[0]}')

!pip install streamlit
!pip install pyngrok

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# import numpy as np
# import pickle
# 
# # Load your saved model
# with open('credit_score_model.pkl', 'rb') as model_file:
#     model = pickle.load(model_file)
# 
# # Define your prediction function
# def predict_credit_score(input_data):
#     data = np.array(input_data).reshape(1, -1)
#     prediction = model.predict(data)
#     return prediction
# 
# st.title('Credit Score Prediction App')
# 
# luxury_spending = st.number_input('Luxury Spending', min_value=0)
# charity_spending = st.number_input('Charity Spending', min_value=0)
# local_business_spending = st.number_input('Small/Local Business Spending', min_value=0)
# groceries = st.number_input('Groceries/Food Spending', min_value=0)
# transportation = st.number_input('Transportation Spending', min_value=0)
# clothing_shopping = st.number_input('Clothing/Shopping Spending', min_value=0)
# electronics = st.number_input('Electronics Spending', min_value=0)
# utilities = st.number_input('Utilities Spending', min_value=0)
# entertainment = st.number_input('Entertainment Spending', min_value=0)
# other_services = st.number_input('Other Services Spending', min_value=0)
# 
# if st.button('Predict Credit Score'):
#     input_data = [
#         luxury_spending,
#         charity_spending,
#         local_business_spending,
#         groceries,
#         transportation,
#         clothing_shopping,
#         electronics,
#         utilities,
#         entertainment,
#         other_services
#     ]
#     prediction = predict_credit_score(input_data)
#     st.success(f'Predicted Credit Score: {prediction[0]}')
#

!pip install pyngrok

from pyngrok import ngrok

# Run Streamlit app in background
!streamlit run app.py &

# Create a tunnel to the Streamlit app
public_url = ngrok.connect(port='8501')
print(f"Streamlit App URL: {public_url}")

pip install gradio numpy pickle-mixin

import gradio as gr
import numpy as np
import pickle

# Load your saved model
with open('credit_score_model.pkl', 'rb') as model_file:
    model = pickle.load(model_file)

# Define your prediction function
def predict_credit_score(luxury_spending, charity_spending, local_business_spending, groceries, transportation, clothing_shopping, electronics, utilities, entertainment, other_services):
    input_data = [
        luxury_spending,
        charity_spending,
        local_business_spending,
        groceries,
        transportation,
        clothing_shopping,
        electronics,
        utilities,
        entertainment,
        other_services
    ]
    data = np.array(input_data).reshape(1, -1)
    prediction = model.predict(data)
    return f'Predicted Credit Score: {prediction[0]}'

# Create Gradio interface
inputs = [
    gr.inputs.Number(label="Luxury Spending"),
    gr.inputs.Number(label="Charity Spending"),
    gr.inputs.Number(label="Small/Local Business Spending"),
    gr.inputs.Number(label="Groceries/Food Spending"),
    gr.inputs.Number(label="Transportation Spending"),
    gr.inputs.Number(label="Clothing/Shopping Spending"),
    gr.inputs.Number(label="Electronics Spending"),
    gr.inputs.Number(label="Utilities Spending"),
    gr.inputs.Number(label="Entertainment Spending"),
    gr.inputs.Number(label="Other Services Spending")
]

output = gr.outputs.Textbox(label="Credit Score Prediction")

gr.Interface(
    fn=predict_credit_score,
    inputs=inputs,
    outputs=output,
    title="Credit Score Prediction App",
    description="Enter your spending details to predict your credit score."
).launch()

import gradio as gr
import numpy as np
import pickle

# ... (rest of your code remains the same)

# Create Gradio interface
inputs = [
    gr.Number(label="Luxury Spending"),  # Changed from gr.inputs.Number
    gr.Number(label="Charity Spending"),  # Changed from gr.inputs.Number
    gr.Number(label="Small/Local Business Spending"),  # Changed from gr.inputs.Number
    gr.Number(label="Groceries/Food Spending"),  # Changed from gr.inputs.Number
    gr.Number(label="Transportation Spending"),  # Changed from gr.inputs.Number
    gr.Number(label="Clothing/Shopping Spending"),  # Changed from gr.inputs.Number
    gr.Number(label="Electronics Spending"),  # Changed from gr.inputs.Number
    gr.Number(label="Utilities Spending"),  # Changed from gr.inputs.Number
    gr.Number(label="Entertainment Spending"),  # Changed from gr.inputs.Number
    gr.Number(label="Other Services Spending")  # Changed from gr.inputs.Number
]

output = gr.Textbox(label="Credit Score Prediction")  # Changed from gr.outputs.Textbox

# ... (rest of your code remains the same)

!pip install dask

!python app.py

!streamlit run app.py [ARGUMENTS]

from google.colab import files

# Download the model file
files.download('model.pkl')

import pickle
with open('model.pkl', 'wb') as model_file:
    pickle.dump(model, model_file)